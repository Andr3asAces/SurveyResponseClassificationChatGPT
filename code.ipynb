{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Survey answer classification with ChatGPT</h1>\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    <img src=\"./data/ChatGPT_logo.png\" alt=\"ChatGPT images\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "### In this project I use the OpenAI API to request the help of ChatGPT to:\n",
    "\n",
    "-   Classify survey responses to topics\n",
    "-   Sentiment analysis\n",
    "-   Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = 'your_api'\n",
    "\n",
    "# get one here: https://platform.openai.com/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"): #\"gpt-3.5-turbo\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/member_survey_comments.csv', encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutting the dataset\n",
    "#df = df.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase the text\n",
    "    #text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    #text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    # Remove extra spaces between words\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    # Replace newline characters with spaces\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    # Remove non-ASCII characters\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the function to each content in the 'answer' column\n",
    "df['answer'] = df['answer'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with an empty list\n",
    "concatenated_content = []\n",
    "\n",
    "# Go through each item in the 'content' column\n",
    "for number,item in enumerate(df['answer']):\n",
    "    # Add the numbered item and the content to the list\n",
    "    concatenated_content.append(str(number) + '. ' + str(item))\n",
    "    # Increment the number for the next item\n",
    "\n",
    "# Join the list into a single string with a newline character between each item\n",
    "long_text = '\\n'.join(concatenated_content)\n",
    "\n",
    "# Now book_text is a single string that contains all the content\n",
    "print(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this script, get_completion is a function that sends a prompt to the GPT-3.5-turbo model and receives a response.\n",
    "# The text is split into chunks of step characters, and each chunk is processed separately to generate a response. \n",
    "# Each response is then added to the responses dictionary with the chunk number as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question='Do you have any comments or reflections on the CompanyXâ€™s proposal for cultural renewal?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics= [\"Approval\",\n",
    "\"CompanyX purpose\",\n",
    "\"Concern over future\",\n",
    "\"Concerns over effectiveness\",\n",
    "\"Disappointment in previous processes\",\n",
    "\"Leading by example\",\n",
    "\"Member inclusion\",\n",
    "\"More detail needed\",\n",
    "\"Needs processes\",\n",
    "\"New leadership\",\n",
    "\"Other\",\n",
    "\"Outside input\",\n",
    "\"Speed\",\n",
    "\"Support for staff\",\n",
    "\"Transparency\",\n",
    "\"Trust\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts={\n",
    "    'summary': \"\"\"\n",
    "    <task> Your task is to provide a detailed summary from the members' answers to the survey <question>.<>    \n",
    "    \n",
    "    <question>: <{}><>\n",
    "    \n",
    "    <introduction>\n",
    "    Our members will be at the heart of shaping the future direction of the CompanyX and our Purpose and Culture Survey is critical in gathering your views to inform our work on what a reimagined, more focused and accountable CompanyX will look like. \n",
    "    Your input to this process is crucial; we need your insight and expertise to drive forward the changes required.\n",
    "    We will share a prospectus, updating you on this work before each member organisation will get to vote on these proposals at the EGM.\n",
    "    <>\n",
    "    \n",
    "    Make sure you read the <introduction>, <question> before you attempt to do the <task>. \n",
    "    \n",
    "    Summarize the <answers> given by member participants to the survey, delimited by triple backticks, in at most 100 words. \n",
    "\n",
    "    <answers>: ```{}```\n",
    "    \"\"\",\n",
    "    \n",
    "    'sentiment' :\"\"\"\n",
    "   \n",
    "    <task> Your tasks are:\n",
    "    1. Evaluate the sentiment of the member's answer to the survey <question>..\n",
    "    2. Explain decision outcome of <task> 1. Stick to less than 25 words.\n",
    "    Give your response 2 outputs in a json format. \n",
    "    <question>: <{}><>\n",
    "    \n",
    "    <introduction>\n",
    "    Our members will be at the heart of shaping the future direction of the CompanyX and our Purpose and Culture Survey is critical in gathering your views to inform our work on what a reimagined, more focused and accountable CompanyX will look like. \n",
    "    Your input to this process is crucial; we need your insight and expertise to drive forward the changes required.\n",
    "    We will share a prospectus, updating you on this work before each member organisation will get to vote on these proposals at the EGM.\n",
    "    <>\n",
    "    \n",
    "    Make sure you read the <introduction>, <question> before you attempt to do the <task>. \n",
    "    \n",
    "    Limit your respone to the <task> in one word 'positive','negative','neutral'.\n",
    "    \n",
    "    Member's <answer> to the survey is delimited by triple backticks.\n",
    "\n",
    "    <answer>: ```{}```\n",
    "    \"\"\",\n",
    "    \n",
    " 'classification' :\"\"\"\n",
    "    \n",
    "    <task> Your tasks are:\n",
    "    1. Classify the <answer> given by the member participant as a response to survey <question>. \n",
    "    Read the set of <topics> listed here:{}. Choose one that is the most likely that the <answer> belongs to.\n",
    "    2. Explain decision outcome of <task> 1. Stick to less than 25 words.\n",
    "    Give your response in a json format with two keys 'topic' and 'explanation'. \n",
    "    Make sure your response spells the <topics> exactly the same.\n",
    "    <>    \n",
    "    \n",
    "    <question>: <{}><>\n",
    "    \n",
    "    <introduction>\n",
    "    Our members will be at the heart of shaping the future direction of the CompanyX and our Purpose and Culture Survey is critical in gathering your views to inform our work on what a reimagined, more focused and accountable CompanyX will look like. \n",
    "    Your input to this process is crucial; we need your insight and expertise to drive forward the changes required.\n",
    "    We will share a prospectus, updating you on this work before each member organisation will get to vote on these proposals at the EGM.\n",
    "    <>\n",
    "    \n",
    "    Make sure you read the <introduction>, <question> and the list of <topics> before you attempt to do the <task>. \n",
    "    \n",
    "    Member's <answer> to the survey is delimited by triple backticks.\n",
    "\n",
    "    <answer>: ```{}```\n",
    "    \n",
    "    \"\"\"\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***I tried to follow best practices and guidelines but it is by no means perfect. Feel free to play around with the prompts.***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sentiment prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompts['sentiment']\n",
    "text   = df['answer'][0]\n",
    "\n",
    "# Generate a response from the model\n",
    "response = get_completion(prompt.format(question,text))\n",
    "print(response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test classification prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt   = prompts['classification']\n",
    "response = get_completion(prompt.format(topics,question,text))\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test summary prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "chunks   = textwrap.wrap(long_text, 200) # 500 characters\n",
    "prompt   = prompts['summary']\n",
    "response = get_completion(prompt.format(question,chunks[0]))\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trick: To fix and avoid limit rate request errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tenacity import retry, wait_exponential\n",
    "\n",
    "@retry(wait=wait_exponential(multiplier=1, min=2, max=30))\n",
    "def delayed_completion(**kwargs):\n",
    "    \"\"\"Call the Completion API with retry mechanism.\"\"\"\n",
    "\n",
    "    # Call the Completion API and return the result\n",
    "    return openai.ChatCompletion.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut the dataset\n",
    "# df = df.loc[0:60,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing sentiment and classification - one at time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this must not exceed.\n",
    "rate_limit_per_minute = 60\n",
    "delay                 = 60.0 / rate_limit_per_minute\n",
    "\n",
    "## choose which prompts to run.\n",
    "prompt_list               = ['sentiment','classification']\n",
    "responses_per_prompt_dict = {}\n",
    "\n",
    "# request statistics\n",
    "start_time            = time.time()\n",
    "request_count         = 0\n",
    "start_time_per_request = time.time()\n",
    "time_taken_per_request = []\n",
    "\n",
    "for prompt_name in prompt_list:\n",
    "    responses_per_prompt  = []\n",
    "    print(f'{prompt_name}')\n",
    "    for text in df.answer:\n",
    "        if prompt_name=='sentiment':\n",
    "            prompt = prompts[prompt_name].format(question,text)\n",
    "        if prompt_name=='classification':\n",
    "            prompt = prompts[prompt_name].format(topics,text)#question,text)\n",
    "            \n",
    "        request_count +=1\n",
    "\n",
    "        if request_count%60==0:\n",
    "            end_time            = time.time()  # Store the current time\n",
    "            time_taken          = round(end_time - start_time,0)  # Calculate the time taken\n",
    "            requests_per_minute = round(60 * request_count / time_taken ,0) # Calculate the requests per minute\n",
    "\n",
    "            \n",
    "            print(f\"Made {request_count} requests in {time_taken} seconds ({requests_per_minute} RPM)\")\n",
    "\n",
    "            start_time    = time.time()  # Reset the start time\n",
    "            request_count = 0  # Reset the counter       \n",
    "\n",
    "        # Sleep for the delay\n",
    "        time.sleep(delay)\n",
    "\n",
    "        # Generate a response from the model with retry mechanism\n",
    "        response = delayed_completion(\n",
    "                    model=\"gpt-3.5-turbo\", \n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0)\n",
    "        \n",
    "        # store the time taken per request/response\n",
    "        time_taken_per_request.append(round(time.time()-start_time_per_request,0))\n",
    "        start_time_per_request = time.time()\n",
    "        \n",
    "        # append each response\n",
    "        responses_per_prompt.append(response)\n",
    "\n",
    "    #store all responses for each prompt\n",
    "    responses_per_prompt_dict[prompt_name] = responses_per_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI returns the data in JSON\n",
    "print(prompts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_per_prompt_dict['classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.loads(response.choices[0].message[\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the response data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Create a list to store the data\n",
    "data = []\n",
    "prompt_name ='classification'\n",
    "# Iterate over the responses\n",
    "for i,response in enumerate(responses_per_prompt_dict[str(prompt_name)]):\n",
    "    temp = json.loads(response.choices[0].message[\"content\"])\n",
    "    # Extract the required information\n",
    "    item = {\n",
    "        \"id\": response.id,\n",
    "        \"model\": response.model,\n",
    "        \"created\": response.created,\n",
    "        \"finish_reason\": response.choices[0].finish_reason,\n",
    "        \"topic\": temp['topic'],\n",
    "        \"explanation\": temp['explanation'],\n",
    "        \"completion_tokens\": response.usage[\"completion_tokens\"],\n",
    "        \"prompt_tokens\": response.usage[\"prompt_tokens\"],\n",
    "        \"request_tokens\": response.usage[\"total_tokens\"],\n",
    "        \"completion_cost\": response.usage[\"completion_tokens\"]*0.002/1000,\n",
    "        \"prompt_cost\": response.usage[\"prompt_tokens\"]*0.002/1000,\n",
    "        \"request_cost\": response.usage[\"total_tokens\"]*0.002/1000,\n",
    "        \"time_taken_per_request\":time_taken_per_request[i]\n",
    "    }\n",
    "    \n",
    "    # Add the item to the data list\n",
    "    data.append(item)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df_responses = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store to csv\n",
    "df_responses.to_csv(f'{prompt_name}.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate ChatGPT predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels\n",
    "le.fit(topics)\n",
    "\n",
    "# Use the fitted LabelEncoder to transform your features\n",
    "true_encoded      = le.transform(df_responses['topic'])\n",
    "predicted_encoded = le.transform(df['topic'])\n",
    "\n",
    "print(accuracy_score(true_encoded,predicted_encoded))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(true_encoded, predicted_encoded)\n",
    "\n",
    "# Use seaborn to plot the confusion matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses['classification'].isin(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses['classification'][df_responses['classification']=='Concern over trust']='Concerns over trust'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses['classification'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Concerns over trust\" model hallucinates a category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be 0\n",
    "sum(~df_responses['classification'].isin(topics))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "\n",
    "#text        = response.choices[0].message[\"content\"]\n",
    "tokens      = encoding.encode(long_text)\n",
    "token_count = len(tokens)\n",
    "\n",
    "print(f\"The text is {token_count} tokens long.\")\n",
    "print(f\"The text is {len(encoding.decode(tokens))} words long.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoding.encode(prompts[prompt_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying different chunk sizes and temperature values for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming long_text is the text you want to summarize\n",
    "import textwrap\n",
    "\n",
    "# Long text\n",
    "text = long_text\n",
    "\n",
    "# Split the text into chunks of \n",
    "\n",
    "chunk_size    = [len(long_text)/2] # [4096, len(long_text)/2, 500]\n",
    "temperatures  = [0.2] #[1, 0.5, 0.2]\n",
    "\n",
    "for temp in temperatures:\n",
    "    for size in chunk_size:\n",
    "    \n",
    "        chunks     = textwrap.wrap(long_text, size)\n",
    "\n",
    "        summaries            = []\n",
    "        responses_per_prompt = []\n",
    "        prompt_name          = 'summary'\n",
    "\n",
    "        print(f'Size of each chunk is {size} characters')\n",
    "        for i,chunk in enumerate(chunks):\n",
    "            # Create the prompt for each chunk\n",
    "            prompt = prompts[prompt_name].format(question,chunk)\n",
    "            print(f'Chunk {i} - Tokens sent: {len(encoding.encode(prompt))}')\n",
    "            # Generate a summary from the model\n",
    "            response = delayed_completion(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temp\n",
    "            )\n",
    "            \n",
    "            # Extract the content from the response\n",
    "            summary = response['choices'][0]['message']['content']\n",
    "            \n",
    "            # Add the summary to the list\n",
    "            summaries.append(summary)\n",
    "            \n",
    "            # append each response\n",
    "            responses_per_prompt.append(response)\n",
    "\n",
    "        # store all responses for each prompt\n",
    "        #responses_per_prompt_dict[f'{prompt_name}_chunk_size_{size}_temperature_{temp}'] = responses_per_prompt\n",
    "        responses_per_prompt_dict[f'{prompt_name}'] = responses_per_prompt\n",
    "        # Combine the summaries\n",
    "        final_summary = \" \".join(summaries)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store to csv\n",
    "df_responses.to_csv(f'{prompt_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_per_prompt_dict.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to calculate the cost before sending the requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total tokens calculator\n",
    "\n",
    "def tokens_calculator(text, n_chunks, n_temperatures, prompt_size=100):\n",
    "    # Encode the text\n",
    "    tokens      = encoding.encode(text)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Calculate average tokens per call\n",
    "    avg_tokens_per_call = (token_count + prompt_size) / n_chunks\n",
    "\n",
    "    # Calculate total tokens\n",
    "    total_tokens = avg_tokens_per_call * n_chunks * n_temperatures\n",
    "\n",
    "    return total_tokens\n",
    "\n",
    "chunk_size    = [4096, len(long_text)/2, 500]\n",
    "temperatures  = [1, 0.5, 0.2]\n",
    "tokens_calculator(text=text, n_chunks=len(chunk_size), n_temperatures=len(temperatures)) * 0.002/1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def cost_calculator(text, n_chunks, n_temperatures, prompt_size=100, price=0.002):\n",
    "    total_tokens = tokens_calculator(text, n_chunks, n_temperatures, prompt_size)\n",
    "    return f'{round(total_tokens * price/1000,3)}$'\n",
    "\n",
    "cost_calculator(text=text, n_chunks=len(chunk_size), n_temperatures=len(temperatures))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This project serves as an introduction to prompt engineering while using ChatGPT to perform machine learning tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
